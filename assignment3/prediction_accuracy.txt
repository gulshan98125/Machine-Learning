part A: (14200 nodes)

pred_train = [0.7809444444444444, 0.8293333333333334, 0.8297222222222222, 0.8302222222222222, 0.8303888888888888, 0.8312222222222222, 0.8318888888888889, 0.8323333333333334, 0.8328333333333333, 0.8333888888888888, 0.8338888888888889, 0.8345, 0.8357222222222223, 0.8363333333333334, 0.8372777777777778, 0.8377222222222223, 0.8383333333333334, 0.8387777777777777, 0.8393888888888889, 0.8401111111111111, 0.8408333333333333, 0.8413888888888889, 0.8419444444444445, 0.8425555555555555, 0.8430555555555556, 0.8441111111111111, 0.8449444444444445, 0.8456666666666667, 0.8459444444444445, 0.8463888888888889, 0.8471666666666666, 0.8476111111111111, 0.8479444444444444, 0.8481111111111111, 0.8486111111111111, 0.8497777777777777, 0.8503333333333334, 0.8505, 0.8518888888888889, 0.8526111111111111, 0.8535555555555555, 0.8540555555555556, 0.8547222222222223, 0.8556111111111111, 0.8563888888888889, 0.8571111111111112, 0.858, 0.8588333333333333, 0.8592777777777778, 0.8603333333333333, 0.8612222222222222, 0.8618888888888889, 0.8626666666666667, 0.8631111111111112, 0.8637777777777778, 0.8646111111111111, 0.8650555555555556, 0.8654444444444445, 0.8662222222222222, 0.8665555555555555, 0.8676111111111111, 0.868, 0.8686111111111111, 0.8691111111111111, 0.8696666666666667, 0.8703888888888889, 0.8710555555555556, 0.8714444444444445, 0.8721666666666666, 0.873, 0.8745555555555555, 0.8753333333333333, 0.8763888888888889, 0.8777777777777778, 0.8788888888888889, 0.8797777777777778, 0.8815555555555555, 0.8823888888888889, 0.8833333333333333, 0.8841111111111111, 0.8845555555555555, 0.8852222222222222, 0.8857777777777778, 0.8864444444444445, 0.8867777777777778, 0.8870555555555556, 0.8872777777777778, 0.8877222222222222, 0.8881111111111111, 0.8883333333333333, 0.8887777777777778, 0.8890555555555556, 0.8897777777777778, 0.8903888888888889, 0.8907222222222222, 0.8918888888888888, 0.8925555555555555, 0.8936666666666667, 0.8940555555555556, 0.8950555555555556, 0.8957777777777778, 0.8965, 0.897, 0.8979444444444444, 0.8985, 0.8998333333333334, 0.9007222222222222, 0.9011111111111111, 0.903, 0.9051111111111111, 0.9069444444444444, 0.9085, 0.9121666666666667, 0.9132777777777777, 0.9138333333333334, 0.9151111111111111, 0.916, 0.9181666666666667, 0.9192777777777777, 0.9203333333333333, 0.9215, 0.9223888888888889, 0.9235555555555556, 0.9254444444444444, 0.9269444444444445, 0.9278888888888889, 0.9291111111111111, 0.9303888888888889, 0.9311111111111111, 0.9325, 0.9333333333333333, 0.9347777777777778, 0.9357222222222222, 0.9370555555555555, 0.9378888888888889, 0.9385555555555556, 0.9398333333333333, 0.9409444444444445, 0.9415, 0.9426666666666667, 0.944, 0.9447777777777778, 0.9452222222222222]

pred_test = [0.7811666666666667, 0.8073333333333333, 0.806, 0.806, 0.8055, 0.8046666666666666, 0.8043333333333333, 0.8041666666666667, 0.8033333333333333, 0.8025, 0.8021666666666667, 0.8011666666666667, 0.7993333333333333, 0.7961666666666667, 0.7956666666666666, 0.7951666666666667, 0.794, 0.7931666666666667, 0.7928333333333333, 0.7926666666666666, 0.7901666666666667, 0.7906666666666666, 0.7896666666666666, 0.7895, 0.79, 0.7883333333333333, 0.7886666666666666, 0.7875, 0.7868333333333334, 0.7856666666666666, 0.7846666666666666, 0.7845, 0.7838333333333334, 0.783, 0.7821666666666667, 0.7811666666666667, 0.7808333333333334, 0.7806666666666666, 0.781, 0.7806666666666666, 0.7801666666666667, 0.7798333333333334, 0.7798333333333334, 0.7783333333333333, 0.7778333333333334, 0.777, 0.7761666666666667, 0.7748333333333334, 0.7743333333333333, 0.7735, 0.7728333333333334, 0.771, 0.7696666666666667, 0.769, 0.768, 0.7663333333333333, 0.7653333333333333, 0.7641666666666667, 0.7636666666666667, 0.7635, 0.7615, 0.7611666666666667, 0.7608333333333334, 0.7595, 0.7588333333333334, 0.7583333333333333, 0.7566666666666667, 0.7563333333333333, 0.7556666666666667, 0.7548333333333334, 0.7535, 0.7523333333333333, 0.7523333333333333, 0.7521666666666667, 0.7511666666666666, 0.7496666666666667, 0.75, 0.7498333333333334, 0.7501666666666666, 0.7495, 0.749, 0.7483333333333333, 0.7473333333333333, 0.746, 0.7451666666666666, 0.7448333333333333, 0.7445, 0.7443333333333333, 0.744, 0.7441666666666666, 0.7441666666666666, 0.744, 0.7425, 0.742, 0.7406666666666667, 0.7388333333333333, 0.7381666666666666, 0.7373333333333333, 0.7371666666666666, 0.7366666666666667, 0.7361666666666666, 0.735, 0.7351666666666666, 0.733, 0.7331666666666666, 0.7341666666666666, 0.7335, 0.7333333333333333, 0.7333333333333333, 0.7325, 0.732, 0.7318333333333333, 0.7308333333333333, 0.731, 0.7306666666666667, 0.7316666666666667, 0.7316666666666667, 0.7316666666666667, 0.7313333333333333, 0.7323333333333333, 0.7326666666666667, 0.7333333333333333, 0.7326666666666667, 0.7328333333333333, 0.7328333333333333, 0.732, 0.7311666666666666, 0.731, 0.731, 0.7301666666666666, 0.7306666666666667, 0.7298333333333333, 0.7298333333333333, 0.7296666666666667, 0.73, 0.7291666666666666, 0.728, 0.7275, 0.7271666666666666, 0.727, 0.7271666666666666, 0.727, 0.7271666666666666]

pred_val = [0.77, 0.803, 0.802, 0.801, 0.8003333333333333, 0.7993333333333333, 0.7985, 0.7983333333333333, 0.7976666666666666, 0.7966666666666666, 0.7963333333333333, 0.7956666666666666, 0.7941666666666667, 0.7903333333333333, 0.79, 0.7895, 0.7893333333333333, 0.7883333333333333, 0.7875, 0.786, 0.7851666666666667, 0.7841666666666667, 0.7835, 0.7826666666666666, 0.7823333333333333, 0.7825, 0.782, 0.7815, 0.7815, 0.7808333333333334, 0.7805, 0.78, 0.7798333333333334, 0.7791666666666667, 0.7781666666666667, 0.7768333333333334, 0.7771666666666667, 0.7766666666666666, 0.7773333333333333, 0.7766666666666666, 0.776, 0.776, 0.7765, 0.7755, 0.7745, 0.7725, 0.7723333333333333, 0.7705, 0.77, 0.7695, 0.769, 0.7675, 0.7658333333333334, 0.7651666666666667, 0.7646666666666667, 0.762, 0.7616666666666667, 0.7603333333333333, 0.7596666666666667, 0.7595, 0.758, 0.7578333333333334, 0.758, 0.7563333333333333, 0.7553333333333333, 0.7553333333333333, 0.7535, 0.7536666666666667, 0.7533333333333333, 0.753, 0.7521666666666667, 0.7505, 0.7488333333333334, 0.7491666666666666, 0.7485, 0.7488333333333334, 0.7498333333333334, 0.7495, 0.7493333333333333, 0.7486666666666667, 0.7478333333333333, 0.7473333333333333, 0.7468333333333333, 0.7456666666666667, 0.7451666666666666, 0.745, 0.7448333333333333, 0.7448333333333333, 0.7448333333333333, 0.7443333333333333, 0.7441666666666666, 0.744, 0.7415, 0.7413333333333333, 0.7411666666666666, 0.7396666666666667, 0.739, 0.738, 0.7383333333333333, 0.738, 0.7383333333333333, 0.7373333333333333, 0.7366666666666667, 0.7361666666666666, 0.7351666666666666, 0.7335, 0.7333333333333333, 0.7323333333333333, 0.7311666666666666, 0.7305, 0.7301666666666666, 0.7313333333333333, 0.733, 0.7333333333333333, 0.7328333333333333, 0.733, 0.733, 0.7318333333333333, 0.7318333333333333, 0.7323333333333333, 0.7321666666666666, 0.7325, 0.733, 0.7335, 0.7341666666666666, 0.7333333333333333, 0.7338333333333333, 0.733, 0.7323333333333333, 0.7321666666666666, 0.7321666666666666, 0.732, 0.732, 0.7313333333333333, 0.7305, 0.7298333333333333, 0.7291666666666666, 0.7281666666666666, 0.7275, 0.7268333333333333, 0.7265, 0.7261666666666666, 0.726]

x = np.arange(0,14300,100)

----------------------------------------------------------------------------------
part B:


val_acc = [0.7315, 0.7988333333333333, 0.807, 0.8136666666666666, 0.8171666666666667, 0.8181666666666667, 0.8196666666666667, 0.8201666666666667, 0.8203333333333334, 0.8203333333333334]
test_acc = [0.733, 0.758, 0.759, 0.7633333333333333, 0.7651666666666667, 0.7643333333333333, 0.766, 0.7658333333333334, 0.766, 0.766]
train_acc = [0.9466666666666667, 0.9067777777777778, 0.8935, 0.8860555555555556, 0.8836666666666667, 0.8816666666666667, 0.8809444444444444, 0.8808333333333334, 0.8806666666666667, 0.8806111111111111]

num_nodes = [14790, 12616, 11288, 10310, 9826, 9578, 9435, 9323, 9281, 9269]
//num_nodes = [14283, 12166, 10871, 9930, 9458, 9215, 9075, 8967, 8926, 8914]

plt.plot(num_nodes, val_acc,markersize=2, color='b',label="validation data")
plt.plot(num_nodes, train_acc,markersize=2, color='m',label="training data")
plt.plot(num_nodes, test_acc,markersize=2, color='g',label="test data")
plt.plot(num_nodes, val_acc,'ro',markersize=5, color='b')
plt.plot(num_nodes, train_acc,'ro',markersize=5, color='m')
plt.plot(num_nodes, test_acc,'ro',markersize=5, color='g')
plt.gca().invert_xaxis()
plt.legend()
plt.show()
--------------------------------------------------------------------------------------

part C:

pred_train = [0.7809444444444444, 0.8287777777777777, 0.8287777777777777, 0.8288333333333333, 0.8288333333333333, 0.8288888888888889, 0.8288888888888889, 0.8289444444444445, 0.8290555555555555, 0.8291111111111111, 0.8291111111111111, 0.8291111111111111, 0.8291666666666667, 0.8291666666666667, 0.8292222222222222, 0.8292777777777778, 0.8293333333333334, 0.8294444444444444, 0.8295, 0.8296111111111111, 0.8297222222222222, 0.8308888888888889, 0.8323888888888888, 0.8333333333333334, 0.8345, 0.8358888888888889, 0.8367777777777777, 0.8381111111111111, 0.8390555555555556, 0.8402222222222222, 0.8408333333333333, 0.8415555555555555, 0.8422777777777778, 0.8440555555555556, 0.8451111111111111, 0.8462777777777778, 0.8476111111111111, 0.8487777777777777, 0.8502222222222222, 0.8515, 0.8523333333333334, 0.8533333333333334, 0.8536666666666667, 0.8547777777777777, 0.8557222222222223, 0.8565555555555555, 0.8575555555555555, 0.8584444444444445, 0.859, 0.8597222222222223, 0.8606111111111111, 0.8614444444444445, 0.8626666666666667, 0.8633888888888889, 0.865, 0.8660555555555556, 0.8669444444444444, 0.8680555555555556, 0.8695555555555555, 0.8710555555555556, 0.8721666666666666, 0.8729444444444444, 0.8738888888888889, 0.8752222222222222, 0.876, 0.8771666666666667, 0.8787222222222222, 0.8807777777777778, 0.8825555555555555, 0.8838333333333334, 0.885, 0.8862222222222222, 0.8875, 0.8887777777777778, 0.8896111111111111, 0.8909444444444444, 0.893, 0.8942222222222223, 0.8955, 0.8968888888888888, 0.8982777777777777, 0.8993333333333333, 0.9009444444444444, 0.9020555555555556, 0.9032777777777777, 0.9043333333333333, 0.9062777777777777, 0.9077777777777778, 0.9093888888888889, 0.9104444444444444, 0.9115555555555556, 0.9123333333333333, 0.9131111111111111, 0.9137777777777778, 0.9143888888888889, 0.9153333333333333, 0.9162777777777777, 0.9170555555555555, 0.9177222222222222, 0.9186666666666666, 0.9196666666666666, 0.9207222222222222, 0.9218888888888889, 0.9231111111111111, 0.9238333333333333, 0.9252777777777778, 0.9268333333333333, 0.9275555555555556, 0.9286666666666666, 0.9298888888888889, 0.9313888888888889, 0.9330555555555555, 0.934, 0.9355, 0.9365555555555556, 0.9380555555555555, 0.9392777777777778, 0.9402222222222222, 0.9408888888888889, 0.9417222222222222, 0.9428888888888889, 0.9443333333333334, 0.9462777777777778, 0.9487777777777778, 0.9507777777777778, 0.9528888888888889, 0.9551666666666667, 0.9587222222222223, 0.9605, 0.9623333333333334, 0.964, 0.9662222222222222, 0.9680555555555556, 0.9696111111111111, 0.971, 0.9732777777777778, 0.9753888888888889, 0.9766111111111111, 0.9782777777777778, 0.9800555555555556, 0.9814444444444445, 0.9829444444444444, 0.9842222222222222, 0.9857777777777778, 0.9873333333333333, 0.9884444444444445, 0.9891666666666666, 0.9911111111111112, 0.9922777777777778, 0.9932777777777778, 0.9947222222222222, 0.9953333333333333, 0.9961666666666666, 0.9977222222222222]

pred_test = [0.7811666666666667, 0.8081666666666667, 0.808, 0.8081666666666667, 0.8081666666666667, 0.8081666666666667, 0.8081666666666667, 0.8081666666666667, 0.8085, 0.8085, 0.8085, 0.8085, 0.8085, 0.8085, 0.808, 0.808, 0.808, 0.808, 0.807, 0.8068333333333333, 0.8073333333333333, 0.806, 0.8053333333333333, 0.8048333333333333, 0.8036666666666666, 0.8028333333333333, 0.803, 0.802, 0.8018333333333333, 0.801, 0.7991666666666667, 0.7988333333333333, 0.7975, 0.7953333333333333, 0.7936666666666666, 0.793, 0.7925, 0.7921666666666667, 0.7921666666666667, 0.7921666666666667, 0.7913333333333333, 0.7898333333333334, 0.7888333333333334, 0.7886666666666666, 0.7883333333333333, 0.7885, 0.7868333333333334, 0.7858333333333334, 0.785, 0.7845, 0.7835, 0.7828333333333334, 0.7821666666666667, 0.781, 0.7798333333333334, 0.7786666666666666, 0.7793333333333333, 0.779, 0.7771666666666667, 0.7766666666666666, 0.7753333333333333, 0.7746666666666666, 0.7736666666666666, 0.7726666666666666, 0.7725, 0.7716666666666666, 0.77, 0.7686666666666667, 0.7668333333333334, 0.766, 0.7653333333333333, 0.7638333333333334, 0.7623333333333333, 0.76, 0.7595, 0.7588333333333334, 0.7571666666666667, 0.7561666666666667, 0.756, 0.7556666666666667, 0.7551666666666667, 0.7538333333333334, 0.7515, 0.7511666666666666, 0.7506666666666667, 0.7498333333333334, 0.7483333333333333, 0.7475, 0.747, 0.7465, 0.7451666666666666, 0.7448333333333333, 0.7441666666666666, 0.7438333333333333, 0.743, 0.7426666666666667, 0.742, 0.7415, 0.741, 0.7403333333333333, 0.7401666666666666, 0.739, 0.738, 0.7368333333333333, 0.7366666666666667, 0.7346666666666667, 0.7335, 0.7335, 0.7328333333333333, 0.732, 0.7303333333333333, 0.729, 0.7283333333333334, 0.7266666666666667, 0.726, 0.7253333333333334, 0.7255, 0.7246666666666667, 0.7243333333333334, 0.724, 0.7235, 0.7241666666666666, 0.7236666666666667, 0.7226666666666667, 0.7223333333333334, 0.7211666666666666, 0.721, 0.7196666666666667, 0.7208333333333333, 0.7211666666666666, 0.721, 0.7215, 0.7218333333333333, 0.722, 0.7216666666666667, 0.7208333333333333, 0.7208333333333333, 0.72, 0.7183333333333334, 0.7186666666666667, 0.7185, 0.7188333333333333, 0.7181666666666666, 0.7173333333333334, 0.7168333333333333, 0.717, 0.7165, 0.7168333333333333, 0.7163333333333334, 0.7158333333333333, 0.716, 0.716, 0.7158333333333333, 0.7153333333333334]

pred_val = [0.77, 0.8035, 0.8035, 0.8035, 0.8035, 0.8038333333333333, 0.8038333333333333, 0.804, 0.8036666666666666, 0.8036666666666666, 0.8036666666666666, 0.8036666666666666, 0.8036666666666666, 0.8036666666666666, 0.8036666666666666, 0.8036666666666666, 0.8035, 0.8035, 0.8033333333333333, 0.803, 0.803, 0.802, 0.801, 0.7998333333333333, 0.7986666666666666, 0.7978333333333333, 0.7971666666666667, 0.7958333333333333, 0.7945, 0.7935, 0.7925, 0.7913333333333333, 0.7908333333333334, 0.7886666666666666, 0.7873333333333333, 0.7856666666666666, 0.785, 0.7843333333333333, 0.7833333333333333, 0.7841666666666667, 0.7838333333333334, 0.7825, 0.7816666666666666, 0.781, 0.7793333333333333, 0.7775, 0.7766666666666666, 0.7758333333333334, 0.7743333333333333, 0.7743333333333333, 0.7731666666666667, 0.7725, 0.7713333333333333, 0.7706666666666667, 0.7708333333333334, 0.771, 0.7708333333333334, 0.7708333333333334, 0.7696666666666667, 0.7681666666666667, 0.767, 0.7663333333333333, 0.7648333333333334, 0.763, 0.7623333333333333, 0.7601666666666667, 0.7583333333333333, 0.757, 0.7556666666666667, 0.7555, 0.7545, 0.7531666666666667, 0.7525, 0.7518333333333334, 0.7518333333333334, 0.7505, 0.7495, 0.7493333333333333, 0.7488333333333334, 0.7473333333333333, 0.745, 0.7443333333333333, 0.744, 0.7446666666666667, 0.744, 0.744, 0.7433333333333333, 0.7416666666666667, 0.7421666666666666, 0.742, 0.7415, 0.741, 0.7405, 0.7396666666666667, 0.7381666666666666, 0.7376666666666667, 0.7375, 0.737, 0.7366666666666667, 0.7365, 0.7348333333333333, 0.7343333333333333, 0.7338333333333333, 0.7328333333333333, 0.7326666666666667, 0.731, 0.728, 0.728, 0.7278333333333333, 0.7268333333333333, 0.7268333333333333, 0.7248333333333333, 0.7238333333333333, 0.722, 0.7223333333333334, 0.7226666666666667, 0.7236666666666667, 0.7236666666666667, 0.723, 0.7226666666666667, 0.7215, 0.72, 0.7188333333333333, 0.72, 0.7181666666666666, 0.7188333333333333, 0.72, 0.7226666666666667, 0.7225, 0.722, 0.7206666666666667, 0.721, 0.7201666666666666, 0.72, 0.7203333333333334, 0.7213333333333334, 0.7208333333333333, 0.7205, 0.7198333333333333, 0.7195, 0.7193333333333334, 0.7188333333333333, 0.719, 0.719, 0.7186666666666667, 0.7181666666666666, 0.7173333333333334, 0.7168333333333333, 0.7163333333333334, 0.7153333333333334, 0.7156666666666667, 0.7153333333333334, 0.7148333333333333, 0.7148333333333333]

x = list(np.arange(0,100,5))+list(np.arange(100,13500,100))

------------------------------------------------------------------------------------------------------------------------------------

Part D:

min_samples_split=9, min_samples_leaf=4, max_depth = 10, random_state =0, val_acc = 0.7997, test_acc = 0.8015

--------------------------------------------------------------------------------------------------------------------

Part E:
val_acc = 0.799, test_acc = 0.8085

-------------------------------------

Part F:
(1) (n_estimators=100, max_depth=10,random_state=0,bootstrap=False) = (val: 0.80417, test: 0.80517)
Even though there looks not much increase in accuracy but the significant difference is that it is able
to predict the minority class comparitively well